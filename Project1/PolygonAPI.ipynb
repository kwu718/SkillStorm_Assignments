{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the required imports \n",
    "import requests, json, pandas as pd, os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the api keys from environment \n",
    "load_dotenv()\n",
    "polygon_key = os.environ['polygon_key']\n",
    "serpapi_key = os.environ['serpapi_key']\n",
    "finhub_key = os.environ['finhub_key']\n",
    "alphav_key = os.environ['alphav_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches stock info for NIKE from Jan 1 2023 to Present\n",
    "ticker = 'NKE'\n",
    "endpoint_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/2023-01-01/2024-07-09?adjusted=true&sort=asc&limit=0&apiKey={polygon_key}'\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# Parses JSON string to Python Dictionary\n",
    "parsed = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/62zqq87s4kgflwh21c1mktwh0000gn/T/ipykernel_65913/4130163728.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_nike = pd.concat([df_nike, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Creates data frame with column names\n",
    "df_nike = pd.DataFrame(columns=['Date','Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Transactions'])\n",
    "\n",
    "# Loops through 'results'\n",
    "for row in parsed['results']:\n",
    "    # Extracts the data and creates a new row\n",
    "    new_row = {'Date': row['t'], 'Open': row['o'], 'High': row['h'],\n",
    "                'Low': row['l'], 'Close': row['c'], 'Volume': row['v'], 'VWAP': row['vw'], 'Transactions': row['n']}\n",
    "    # Appends new row to data frame\n",
    "    df_nike = pd.concat([df_nike, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts unix Msec timestamp to month/day/year format\n",
    "df_nike['Date'] = pd.to_datetime(df_nike['Date'], unit='ms').dt.strftime('%m/%d/%y')\n",
    "\n",
    "# Rounds daily low to nears two decimal point\n",
    "df_nike = df_nike.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from secondary source to ensure data accuracy \n",
    "df_nike_nasdaq = pd.read_csv('csv_files/NKE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of '$' symbol and changes type to float\n",
    "df_nike_nasdaq['Close'] = df_nike_nasdaq['Close'].str.replace('$', '').astype(float)\n",
    "df_nike_nasdaq['Open'] = df_nike_nasdaq['Open'].str.replace('$', '').astype(float)\n",
    "df_nike_nasdaq['High'] = df_nike_nasdaq['High'].str.replace('$', '').astype(float)\n",
    "df_nike_nasdaq['Low'] = df_nike_nasdaq['Low'].str.replace('$', '').astype(float)\n",
    "\n",
    "# Rounds all values to 2 decimal places\n",
    "df_nike_nasdaq = df_nike_nasdaq.round(2)\n",
    "\n",
    "# Date is already in the m/d/y format\n",
    "#df_nike_nasdaq['Date'] = pd.to_datetime(df_nike_nasdaq['Date']).dt.strftime('%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverses dataframe so it follows same order as original df\n",
    "df_nike_nasdaq = df_nike_nasdaq[::-1]\n",
    "df_nike_nasdaq = df_nike_nasdaq.reset_index(drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open  High  Low  Close\n",
       "105   0.0  0.32  0.0    0.0"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds if there is any different between the values\n",
    "# Only found a $0.32 difference the 'High' value on 06/05/23\n",
    "# I consider that negligible and don't think that will have any huge affect on my data\n",
    "cols = ['Open', 'High', 'Low', 'Close']\n",
    "df_difference = df_nike[cols] - df_nike_nasdaq[cols]\n",
    "df_difference[(df_difference[['Open', 'Close', 'High', 'Low']] != 0).any(axis=1)]\n",
    "#df_nike.loc[105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in data for Deckers Outdoor Corporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'DECK'\n",
    "endpoint_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/2023-01-01/2024-07-09?adjusted=true&sort=asc&limit=0&apiKey={polygon_key}'\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# Parses JSON string to Python Dictionary\n",
    "parsed = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/62zqq87s4kgflwh21c1mktwh0000gn/T/ipykernel_65913/2587989883.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_decker = pd.concat([df_decker, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Creates data frame with column names\n",
    "df_decker = pd.DataFrame(columns=['Date','Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Transactions'])\n",
    "\n",
    "# Loops through 'results'\n",
    "for row in parsed['results']:\n",
    "    # Extracts the data and creates a new row\n",
    "    new_row = {'Date': row['t'], 'Open': row['o'], 'High': row['h'],\n",
    "                'Low': row['l'], 'Close': row['c'], 'Volume': row['v'], 'VWAP': row['vw'], 'Transactions': row['n']}\n",
    "    # Appends new row to data frame\n",
    "    df_decker = pd.concat([df_decker, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts unix Msec timestamp to month/day/year format\n",
    "df_decker['Date'] = pd.to_datetime(df_decker['Date'], unit='ms').dt.strftime('%m/%d/%y')\n",
    "\n",
    "# Rounds daily low to nears two decimal point\n",
    "df_decker = df_decker.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decker_nasdaq = pd.read_csv('csv_files/DECK.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of '$' symbol and changes type to float\n",
    "df_decker_nasdaq['Close'] = df_decker_nasdaq['Close'].str.replace('$', '').astype(float)\n",
    "df_decker_nasdaq['Open'] = df_decker_nasdaq['Open'].str.replace('$', '').astype(float)\n",
    "df_decker_nasdaq['High'] = df_decker_nasdaq['High'].str.replace('$', '').astype(float)\n",
    "df_decker_nasdaq['Low'] = df_decker_nasdaq['Low'].str.replace('$', '').astype(float)\n",
    "\n",
    "# Rounds all values to 2 decimal places\n",
    "df_decker_nasdaq = df_decker_nasdaq.round(2)\n",
    "\n",
    "# Date is already in the m/d/y format\n",
    "# df_decker_nasdaq['Date'] = pd.to_datetime(df_decker_nasdaq['Date']).dt.strftime('%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverses dataframe so it follows same order as original df\n",
    "df_decker_nasdaq = df_decker_nasdaq[::-1]\n",
    "df_decker_nasdaq = df_decker_nasdaq.reset_index(drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Open, High, Low, Close, Volume]\n",
       "Index: []"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Found no inconsistencies when compared to data from Nasdaq\n",
    "cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "df_difference = df_decker[cols] - df_decker_nasdaq[cols]\n",
    "df_difference[(df_difference[['Open', 'Close', 'High', 'Low']] != 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'SPOT'\n",
    "endpoint_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/2023-01-01/2024-07-09?adjusted=true&sort=asc&limit=0&apiKey={polygon_key}'\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# Parses JSON string to Python Dictionary\n",
    "parsed = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/62zqq87s4kgflwh21c1mktwh0000gn/T/ipykernel_65913/1895166580.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_spotify = pd.concat([df_spotify, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Creates data frame with column names\n",
    "df_spotify = pd.DataFrame(columns=['Date','Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Transactions'])\n",
    "\n",
    "# Loops through 'results'\n",
    "for row in parsed['results']:\n",
    "    # Extracts the data and creates a new row\n",
    "    new_row = {'Date': row['t'], 'Open': row['o'], 'High': row['h'],\n",
    "                'Low': row['l'], 'Close': row['c'], 'Volume': row['v'], 'VWAP': row['vw'], 'Transactions': row['n']}\n",
    "    # Appends new row to data frame\n",
    "    df_spotify = pd.concat([df_spotify, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts unix Msec timestamp to month/day/year format\n",
    "df_spotify['Date'] = pd.to_datetime(df_spotify['Date'], unit='ms').dt.strftime('%m/%d/%y')\n",
    "\n",
    "# Rounds daily low to nears two decimal point\n",
    "df_spotify = df_spotify.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_nasdaq = pd.read_csv('csv_files/SPOT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of '$' symbol and changes type to float\n",
    "df_spotify_nasdaq['Close'] = df_spotify_nasdaq['Close'].str.replace('$', '').astype(float)\n",
    "df_spotify_nasdaq['Open'] = df_spotify_nasdaq['Open'].str.replace('$', '').astype(float)\n",
    "df_spotify_nasdaq['High'] = df_spotify_nasdaq['High'].str.replace('$', '').astype(float)\n",
    "df_spotify_nasdaq['Low'] = df_spotify_nasdaq['Low'].str.replace('$', '').astype(float)\n",
    "\n",
    "# Rounds all values to 2 decimal places\n",
    "df_spotify_nasdaq = df_spotify_nasdaq.round(2)\n",
    "\n",
    "# Date is already in the m/d/y format\n",
    "# df_spotify_nasdaq['Date'] = pd.to_datetime(df_spotify_nasdaq['Date']).dt.strftime('%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverses dataframe so it follows same order as original df\n",
    "df_spotify_nasdaq = df_spotify_nasdaq[::-1]\n",
    "df_spotify_nasdaq = df_spotify_nasdaq.reset_index(drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Open, High, Low, Close, Volume]\n",
       "Index: []"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Found no inconsistencies when compared to data from Nasdaq\n",
    "cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "df_difference = df_spotify[cols] - df_spotify_nasdaq[cols]\n",
    "df_difference[(df_difference[['Open', 'Close', 'High', 'Low']] != 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for Vanguard FTSE Developed Markets ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'VEA'\n",
    "endpoint_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/2023-01-01/2024-07-09?adjusted=true&sort=asc&limit=0&apiKey={polygon_key}'\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# Parses JSON string to Python Dictionary\n",
    "parsed = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/62zqq87s4kgflwh21c1mktwh0000gn/T/ipykernel_65913/3202851404.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_etf = pd.concat([df_etf, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Creates data frame with column names\n",
    "df_etf = pd.DataFrame(columns=['Date','Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Transactions'])\n",
    "\n",
    "# Loops through 'results'\n",
    "for row in parsed['results']:\n",
    "    # Extracts the data and creates a new row\n",
    "    new_row = {'Date': row['t'], 'Open': row['o'], 'High': row['h'],\n",
    "                'Low': row['l'], 'Close': row['c'], 'Volume': row['v'], 'VWAP': row['vw'], 'Transactions': row['n']}\n",
    "    # Appends new row to data frame\n",
    "    df_etf = pd.concat([df_etf, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts unix Msec timestamp to month/day/year format\n",
    "df_etf['Date'] = pd.to_datetime(df_etf['Date'], unit='ms').dt.strftime('%m/%d/%y')\n",
    "\n",
    "# Rounds daily low to nears two decimal point\n",
    "df_etf = df_etf.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_etf_nasdaq = pd.read_csv('csv_files/VEA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price numbers are already in proper format\n",
    "# No need to replace '$' and change to float type\n",
    "\n",
    "# Rounds all values to 2 decimal places\n",
    "df_etf_nasdaq = df_etf_nasdaq.round(2)\n",
    "\n",
    "# Date is already in the m/d/y format\n",
    "#df_etf_nasdaq['Date'] = pd.to_datetime(df_etf_nasdaq['Date']).dt.strftime('%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverses dataframe so it follows same order as original df\n",
    "df_etf_nasdaq = df_etf_nasdaq[::-1]\n",
    "df_etf_nasdaq = df_etf_nasdaq.reset_index(drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open  High  Low  Close\n",
       "101   0.0 -0.01  0.0    0.0"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $0.01 discrepancy for daily high on 05/30/23\n",
    "# Negligible\n",
    "cols = ['Open', 'High', 'Low', 'Close']\n",
    "df_difference = df_etf[cols] - df_etf_nasdaq[cols]\n",
    "df_difference[(df_difference[['Open', 'Close', 'High', 'Low']] != 0).any(axis=1)]\n",
    "#df_etf.loc[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forex GBP/USD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'C:GBPUSD'\n",
    "endpoint_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/2023-01-01/2024-07-09?adjusted=true&sort=asc&apiKey={polygon_key}'\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# Parses JSON string to Python Dictionary\n",
    "parsed = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/62zqq87s4kgflwh21c1mktwh0000gn/T/ipykernel_65913/2559983708.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_forex = pd.concat([df_forex, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Creates data frame with column names\n",
    "df_forex = pd.DataFrame(columns=['Date','Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Transactions'])\n",
    "\n",
    "# Loops through 'results'\n",
    "for row in parsed['results']:\n",
    "    # Extracts the data and creates a new row\n",
    "    new_row = {'Date': row['t'], 'Open': row['o'], 'High': row['h'],\n",
    "                'Low': row['l'], 'Close': row['c'], 'Volume': row['v'], 'VWAP': row['vw'], 'Transactions': row['n']}\n",
    "    # Appends new row to data frame\n",
    "    df_forex = pd.concat([df_forex, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts unix Msec timestamp to month/day/year format\n",
    "df_forex['Date'] = pd.to_datetime(df_forex['Date'], unit='ms').dt.strftime('%m/%d/%y')\n",
    "\n",
    "# Don't want to round values to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical data from Yahoo Finance\n",
    "df_forex_yahoo = pd.read_csv('csv_files/FOREX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats dates from year/month/day to month/day/year\n",
    "df_forex_yahoo['Date'] = pd.to_datetime(df_forex_yahoo['Date']).dt.strftime('%m/%d/%y')\n",
    "\n",
    "#df_forex_yahoo.info()\n",
    "# One row contains null values gets changed to 0\n",
    "df_forex_yahoo = df_forex_yahoo.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index of the dataframes to date so we can subtract properly \n",
    "df_forex = df_forex.set_index('Date', drop='True')\n",
    "df_forex_yahoo = df_forex_yahoo.set_index('Date', drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23-11-14</th>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.021226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24-07-05</th>\n",
       "      <td>1.275460</td>\n",
       "      <td>1.281800</td>\n",
       "      <td>1.275300</td>\n",
       "      <td>1.281300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open      High       Low     Close\n",
       "Date                                            \n",
       "23-11-14 -0.000064  0.001802 -0.000142  0.021226\n",
       "24-07-05  1.275460  1.281800  1.275300  1.281300"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A lot of \n",
    "cols = ['Open', 'High', 'Low', 'Close']\n",
    "df_difference = df_forex[cols] - df_forex_yahoo[cols]\n",
    "\n",
    "# Changed the format to year/month/day so values can be sorted properly\n",
    "df_difference.index = pd.to_datetime(df_difference.index, format='%m/%d/%y').strftime('%y-%m-%d')\n",
    "\n",
    "# A lot of discrepancies with data, but most were very small with only one different surpassing .02\n",
    "# Not all dates had data available across multiple sources (Nasdaq, Yahoo Finance, etc...)\n",
    "df_difference[(df_difference[['Open', 'Close', 'High', 'Low']] >= 0.02).any(axis=1)].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indicies back to original\n",
    "df_forex = df_forex.reset_index()\n",
    "df_forex_yahoo = df_forex_yahoo.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Breakdown\n",
    "- 20k on Nike\n",
    "- 5k on Deckers Outdoor Corporation\n",
    "- 50k on Spotify\n",
    "- 15k on Vanguard FTSE Developed Markets ETF\n",
    "- 10k on GBP/USD Forex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe\n",
    "# Add percent change column based previous days close price\n",
    "df_nike2 = df_nike.copy()\n",
    "df_nike2['Percent Change'] = df_nike['Close'].pct_change().fillna(0)\n",
    "\n",
    "# Calculate daily return\n",
    "list = [0]\n",
    "previous = 20000\n",
    "for val in df_nike2['Percent Change'][1:]:\n",
    "    new = round(val * previous, 2)\n",
    "    list.append(new)\n",
    "    previous = previous + new\n",
    "df_nike2['Return'] = list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe\n",
    "# Add percent change column based previous days close price\n",
    "df_decker2 = df_decker.copy()\n",
    "df_decker2['Percent Change'] = df_decker2['Close'].pct_change().fillna(0)\n",
    "\n",
    "# Calculate daily return\n",
    "list = [0]\n",
    "previous = 5000\n",
    "for val in df_decker2['Percent Change'][1:]:\n",
    "    new = round(val * previous, 2)\n",
    "    list.append(new)\n",
    "    previous = previous + new\n",
    "df_decker2['Return'] = list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe\n",
    "# Add percent change column based previous days close price\n",
    "df_spotify2 = df_spotify.copy()\n",
    "df_spotify2['Percent Change'] = df_spotify2['Close'].pct_change().fillna(0)\n",
    "\n",
    "# Calculate daily return\n",
    "list = [0]\n",
    "previous = 50000\n",
    "for val in df_spotify2['Percent Change'][1:]:\n",
    "    new = round(val * previous, 2)\n",
    "    list.append(new)\n",
    "    previous = previous + new\n",
    "df_spotify2['Return'] = list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe\n",
    "# Add percent change column based previous days close price\n",
    "df_etf2 = df_etf.copy()\n",
    "df_etf2['Percent Change'] = df_etf2['Close'].pct_change().fillna(0)\n",
    "\n",
    "# Calculate daily return\n",
    "list = [0]\n",
    "previous = 15000\n",
    "for val in df_etf2['Percent Change'][1:]:\n",
    "    new = round(val * previous, 2)\n",
    "    list.append(new)\n",
    "    previous = previous + new\n",
    "df_etf2['Return'] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe\n",
    "# Add percent change column based previous days close price\n",
    "df_forex2 = df_forex.copy()\n",
    "df_forex2['Percent Change'] = df_forex2['Close'].pct_change().fillna(0)\n",
    "\n",
    "# Calculate daily return\n",
    "list = [0]\n",
    "previous = 10000\n",
    "for val in df_forex2['Percent Change'][1:]:\n",
    "    new = round(val * previous, 2)\n",
    "    list.append(new)\n",
    "    previous = previous + new\n",
    "df_forex2['Return'] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges all cumulative returns for all 5 investments into single table\n",
    "df_returns = pd.merge(df_forex2[['Date', 'Return']], df_etf2[['Date', 'Return']], on = 'Date', how = 'left', suffixes=(' Forex', ' ETF'))\n",
    "df_returns = df_returns.merge(df_nike2[['Date', 'Return']], on = 'Date', how = 'left')\n",
    "df_returns = df_returns.merge(df_decker2[['Date', 'Return']], on = 'Date', how = 'left')\n",
    "df_returns = df_returns.merge(df_spotify2[['Date', 'Return']], on = 'Date', how = 'left')\n",
    "df_returns = df_returns.fillna(0)\n",
    "\n",
    "# Rename columns to appropriate names\n",
    "df_returns = df_returns.rename(columns={'Return_x': 'Return Nike', 'Return_y': 'Return Deckers', 'Return': 'Return Spotify'})\n",
    "\n",
    "# Sums all returns to one Total Daily Return column\n",
    "df_returns['Total Returns'] = df_returns.sum(axis=1, numeric_only='True')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV files from Dataframes\n",
    "# Load files to SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nike2.index = df_nike2.index + 1\n",
    "df_nike2['Percent Change'] = df_nike2['Percent Change'].round(5)\n",
    "df_nike2.to_csv('Nike_Table.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decker2.index = df_decker2.index + 1\n",
    "df_decker2['Percent Change'] = df_decker2['Percent Change'].round(5)\n",
    "df_decker2.to_csv('Decker_Table.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify2.index = df_spotify2.index + 1\n",
    "df_spotify2['Percent Change'] = df_spotify2['Percent Change'].round(5)\n",
    "df_spotify2.to_csv('Spotify_Table.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_etf2.index = df_etf2.index + 1\n",
    "df_etf2['Percent Change'] = df_etf2['Percent Change'].round(5)\n",
    "df_etf2.to_csv('ETF_Table.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex2.index = df_forex2.index + 1\n",
    "df_forex2['Percent Change'] = df_forex2['Percent Change'].round(5)\n",
    "df_forex2.to_csv('Forex_Table.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns.index = df_returns.index + 1\n",
    "df_returns.to_csv('Returns.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges all dataframes so I can calculate weighted percent change\n",
    "df_pc = pd.merge(df_forex2[['Date', 'Percent Change']], df_etf2[['Date', 'Percent Change']], on = 'Date', how = 'left', suffixes=(' Forex', ' ETF'))\n",
    "df_pc = df_pc.merge(df_nike2[['Date', 'Percent Change']], on = 'Date', how = 'left')\n",
    "df_pc = df_pc.merge(df_decker2[['Date', 'Percent Change']], on = 'Date', how = 'left')\n",
    "df_pc = df_pc.merge(df_spotify2[['Date', 'Percent Change']], on = 'Date', how = 'left')\n",
    "df_pc = df_pc.fillna(0)\n",
    "\n",
    "# Rename columns to appropriate names\n",
    "df_pc = df_pc.rename(columns={'Percent Change_x': 'Percent Change Nike', 'Percent Change_y': 'Percent Change Deckers', 'Percent Change': 'Percent Change Spotify'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(142853.91)"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Returns of Portfolio\n",
    "total_returns = df_returns['Total Returns'].sum()\n",
    "total_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Return Forex</th>\n",
       "      <th>Return ETF</th>\n",
       "      <th>Return Nike</th>\n",
       "      <th>Return Deckers</th>\n",
       "      <th>Return Spotify</th>\n",
       "      <th>Total Returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/23</td>\n",
       "      <td>-17.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-17.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/03/23</td>\n",
       "      <td>-80.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-80.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/04/23</td>\n",
       "      <td>74.73</td>\n",
       "      <td>223.93</td>\n",
       "      <td>414.32</td>\n",
       "      <td>9.12</td>\n",
       "      <td>-347.99</td>\n",
       "      <td>374.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/05/23</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>-170.62</td>\n",
       "      <td>-99.37</td>\n",
       "      <td>66.03</td>\n",
       "      <td>152.63</td>\n",
       "      <td>-174.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>07/04/24</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>07/05/24</td>\n",
       "      <td>48.33</td>\n",
       "      <td>113.74</td>\n",
       "      <td>32.00</td>\n",
       "      <td>46.63</td>\n",
       "      <td>1251.53</td>\n",
       "      <td>1492.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>07/07/24</td>\n",
       "      <td>-5.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-5.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>07/08/24</td>\n",
       "      <td>7.20</td>\n",
       "      <td>-78.20</td>\n",
       "      <td>-400.84</td>\n",
       "      <td>171.88</td>\n",
       "      <td>-1916.97</td>\n",
       "      <td>-2216.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>07/09/24</td>\n",
       "      <td>-23.75</td>\n",
       "      <td>-60.43</td>\n",
       "      <td>-99.37</td>\n",
       "      <td>-175.35</td>\n",
       "      <td>-1318.68</td>\n",
       "      <td>-1677.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Return Forex  Return ETF  Return Nike  Return Deckers  \\\n",
       "0    01/01/23          0.00        0.00         0.00            0.00   \n",
       "1    01/02/23        -17.96        0.00         0.00            0.00   \n",
       "2    01/03/23        -80.03        0.00         0.00            0.00   \n",
       "3    01/04/23         74.73      223.93       414.32            9.12   \n",
       "4    01/05/23       -122.98     -170.62       -99.37           66.03   \n",
       "..        ...           ...         ...          ...             ...   \n",
       "491  07/04/24          6.62        0.00         0.00            0.00   \n",
       "492  07/05/24         48.33      113.74        32.00           46.63   \n",
       "493  07/07/24         -5.79        0.00         0.00            0.00   \n",
       "494  07/08/24          7.20      -78.20      -400.84          171.88   \n",
       "495  07/09/24        -23.75      -60.43       -99.37         -175.35   \n",
       "\n",
       "     Return Spotify  Total Returns  \n",
       "0              0.00           0.00  \n",
       "1              0.00         -17.96  \n",
       "2              0.00         -80.03  \n",
       "3           -347.99         374.11  \n",
       "4            152.63        -174.31  \n",
       "..              ...            ...  \n",
       "491            0.00           6.62  \n",
       "492         1251.53        1492.23  \n",
       "493            0.00          -5.79  \n",
       "494        -1916.97       -2216.93  \n",
       "495        -1318.68       -1677.58  \n",
       "\n",
       "[496 rows x 7 columns]"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cumulative Returns \n",
    "\n",
    "# Shows returns by month\n",
    "df_returns['Date'] = pd.to_datetime(df_returns['Date'], format='%m/%d/%y')\n",
    "month_returns = df_returns.groupby([df_returns.Date.dt.year, df_returns.Date.dt.month]).agg({'Total Returns': 'sum'})\n",
    "\n",
    "# Shows daily return for each stock as well as total return\n",
    "df_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>69150.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>73703.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Total Returns\n",
       "Date               \n",
       "2023       69150.50\n",
       "2024       73703.41"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Annualized Return\n",
    "df_returns['Date'] = pd.to_datetime(df_returns['Date'], format='%m/%d/%y')\n",
    "annual_returns = df_returns.groupby([df_returns.Date.dt.year]).agg({'Total Returns': 'sum'})\n",
    "annual_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.012132606998778292)"
      ]
     },
     "execution_count": 991,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure Volatility of Portfolio\n",
    "df_pc['Weighted Returns'] = (df_pc['Percent Change Nike'] * .20) + (df_pc['Percent Change Deckers'] * .05) + (df_pc['Percent Change Spotify'] * .50) + (df_pc['Percent Change ETF'] * .15) + (df_pc['Percent Change Forex'] * .100)\n",
    "volatility = df_pc['Weighted Returns'].std()\n",
    "volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.210206276578376)"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# Measure Sharpe Ratio\n",
    "\n",
    "# Calculate Scaled holding period\n",
    "hp= ((100000 + df_returns['Total Returns'].sum()) / 100000) - 1\n",
    "shp = hp * (365/496)\n",
    "\n",
    "# Calculate annualized volatility \n",
    "av = volatility * math.sqrt(261)\n",
    "\n",
    "sharpe_ratio = (shp-.03)/av\n",
    "sharpe_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Nike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1004], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Total Return\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_nike2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReturn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Monthly Returns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_returns\u001b[38;5;241m.\u001b[39mgroupby([df_returns\u001b[38;5;241m.\u001b[39mDate\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear, df_returns\u001b[38;5;241m.\u001b[39mDate\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth])\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturn Nike\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Total Return\n",
    "print(df_nike2['Return'].sum())\n",
    "\n",
    "# Monthly Returns\n",
    "df_returns.groupby([df_returns.Date.dt.year, df_returns.Date.dt.month]).agg({'Return Nike': 'sum'})\n",
    "\n",
    "# Volatility \n",
    "df_nike2['Percent Change'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.9866551072475059)"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sharpe Ratio\n",
    "\n",
    "hp= ((20000 + df_returns['Return Nike'].sum()) / 20000) - 1\n",
    "shp = hp * (252/365)\n",
    "\n",
    "# Calculate annualized volatility \n",
    "av = df_nike2['Percent Change'].std() * math.sqrt(252)\n",
    "\n",
    "sharpe_ratio = (shp-.03)/av\n",
    "sharpe_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in Nasdaq-100 index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file from Nasdaq. Polygon's data didn't go back far enough.\n",
    "df_ndx = pd.read_csv('csv_files/NDX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndx = df_ndx[::-1].reset_index(drop=True)\n",
    "df_ndx['Percent Change'] = df_ndx['Close'].pct_change().fillna(0)\n",
    "df_ndx['Date'] = pd.to_datetime(df_ndx['Date']).dt.strftime('%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates CSV to load into SQL Database\n",
    "df_ndx.index = df_ndx.index + 1\n",
    "df_ndx['Percent Change'] = df_ndx['Percent Change'].round(5)\n",
    "df_ndx.to_csv('NDX_Table.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving 10 day and 100 day average\n",
    "\n",
    "df_nike2['Percent Change'].rolling(window=10).mean()\n",
    "df_ndx['Percent Change'].rolling(window=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5270538608017314)"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beta calculations\n",
    "df_cov = pd.merge(df_ndx[['Date', 'Percent Change']], df_nike2[['Date', 'Percent Change']], on='Date')\n",
    "df_cov = df_cov.drop(columns={'Date'})\n",
    "\n",
    "variance = df_ndx['Percent Change'].var()\n",
    "covariance = df_cov.cov()['Percent Change_x']['Percent Change_y']\n",
    "\n",
    "covariance/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Deckers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.02167833424456855)"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Return\n",
    "df_decker2['Return'].sum()\n",
    "\n",
    "# Monthly Returns\n",
    "df_returns.groupby([df_returns.Date.dt.year, df_returns.Date.dt.month]).agg({'Return Deckers': 'sum'})\n",
    "\n",
    "# Volatility \n",
    "df_decker2['Percent Change'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.7467777832630107)"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sharpe Ratio\n",
    "\n",
    "hp= ((5000 + df_returns['Return Deckers'].sum()) / 5000) - 1\n",
    "shp = hp * (252/365)\n",
    "\n",
    "# Calculate annualized volatility \n",
    "av = df_decker2['Percent Change'].std() * math.sqrt(252)\n",
    "\n",
    "sharpe_ratio = (shp-.03)/av\n",
    "sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving 10 day and 100 day average\n",
    "\n",
    "df_decker2['Percent Change'].rolling(window=10).mean()\n",
    "df_ndx['Percent Change'].rolling(window=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7105130905315041)"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beta calculations\n",
    "df_cov = pd.merge(df_ndx[['Date', 'Percent Change']], df_decker2[['Date', 'Percent Change']], on='Date')\n",
    "df_cov = df_cov.drop(columns={'Date'})\n",
    "\n",
    "variance = df_ndx['Percent Change'].var()\n",
    "covariance = df_cov.cov()['Percent Change_x']['Percent Change_y']\n",
    "\n",
    "covariance/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.023922800784499564)"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Return\n",
    "df_spotify2['Return'].sum()\n",
    "\n",
    "# Monthly Returns\n",
    "df_returns.groupby([df_returns.Date.dt.year, df_returns.Date.dt.month]).agg({'Return Spotify': 'sum'})\n",
    "\n",
    "# Volatility \n",
    "df_spotify2['Percent Change'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.018750392698874)"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sharpe Ratio\n",
    "\n",
    "hp= ((50000 + df_returns['Return Spotify'].sum()) / 50000) - 1\n",
    "shp = hp * (252/365)\n",
    "\n",
    "# Calculate annualized volatility \n",
    "av = df_spotify2['Percent Change'].std() * math.sqrt(252)\n",
    "\n",
    "sharpe_ratio = (shp-.03)/av\n",
    "sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           NaN\n",
       "1           NaN\n",
       "2           NaN\n",
       "3           NaN\n",
       "4           NaN\n",
       "         ...   \n",
       "375    0.000573\n",
       "376    0.001415\n",
       "377    0.003219\n",
       "378    0.003715\n",
       "379    0.004927\n",
       "Name: Percent Change, Length: 380, dtype: float64"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moving 10 day and 100 day average\n",
    "\n",
    "df_spotify2['Percent Change'].rolling(window=10).mean()\n",
    "df_ndx['Percent Change'].rolling(window=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9290956361724463)"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beta calculations\n",
    "df_cov = pd.merge(df_ndx[['Date', 'Percent Change']], df_spotify2[['Date', 'Percent Change']], on='Date')\n",
    "df_cov = df_cov.drop(columns={'Date'})\n",
    "\n",
    "variance = df_ndx['Percent Change'].var()\n",
    "covariance = df_cov.cov()['Percent Change_x']['Percent Change_y']\n",
    "\n",
    "covariance/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Vanguard Developed Markets ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Transactions</th>\n",
       "      <th>Percent Change</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/03/23</td>\n",
       "      <td>42.40</td>\n",
       "      <td>42.64</td>\n",
       "      <td>42.03</td>\n",
       "      <td>42.20</td>\n",
       "      <td>11990004.0</td>\n",
       "      <td>42.23</td>\n",
       "      <td>38830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/04/23</td>\n",
       "      <td>42.85</td>\n",
       "      <td>42.95</td>\n",
       "      <td>42.52</td>\n",
       "      <td>42.83</td>\n",
       "      <td>13229545.0</td>\n",
       "      <td>42.81</td>\n",
       "      <td>42125</td>\n",
       "      <td>0.014929</td>\n",
       "      <td>223.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/05/23</td>\n",
       "      <td>42.42</td>\n",
       "      <td>42.53</td>\n",
       "      <td>42.27</td>\n",
       "      <td>42.35</td>\n",
       "      <td>9324542.0</td>\n",
       "      <td>42.40</td>\n",
       "      <td>30234</td>\n",
       "      <td>-0.011207</td>\n",
       "      <td>-170.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/06/23</td>\n",
       "      <td>42.66</td>\n",
       "      <td>43.55</td>\n",
       "      <td>42.45</td>\n",
       "      <td>43.52</td>\n",
       "      <td>10441417.0</td>\n",
       "      <td>43.26</td>\n",
       "      <td>29984</td>\n",
       "      <td>0.027627</td>\n",
       "      <td>415.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/09/23</td>\n",
       "      <td>43.89</td>\n",
       "      <td>44.14</td>\n",
       "      <td>43.71</td>\n",
       "      <td>43.71</td>\n",
       "      <td>12426293.0</td>\n",
       "      <td>43.95</td>\n",
       "      <td>40155</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>67.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>07/02/24</td>\n",
       "      <td>49.33</td>\n",
       "      <td>49.60</td>\n",
       "      <td>49.26</td>\n",
       "      <td>49.57</td>\n",
       "      <td>9674116.0</td>\n",
       "      <td>49.43</td>\n",
       "      <td>29669</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>39.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>07/03/24</td>\n",
       "      <td>49.81</td>\n",
       "      <td>50.22</td>\n",
       "      <td>49.81</td>\n",
       "      <td>50.16</td>\n",
       "      <td>6989690.0</td>\n",
       "      <td>50.13</td>\n",
       "      <td>20772</td>\n",
       "      <td>0.011902</td>\n",
       "      <td>209.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>07/05/24</td>\n",
       "      <td>50.62</td>\n",
       "      <td>50.62</td>\n",
       "      <td>50.18</td>\n",
       "      <td>50.48</td>\n",
       "      <td>7271284.0</td>\n",
       "      <td>50.40</td>\n",
       "      <td>27871</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>113.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>07/08/24</td>\n",
       "      <td>50.52</td>\n",
       "      <td>50.56</td>\n",
       "      <td>50.23</td>\n",
       "      <td>50.26</td>\n",
       "      <td>8229614.0</td>\n",
       "      <td>50.35</td>\n",
       "      <td>29295</td>\n",
       "      <td>-0.004358</td>\n",
       "      <td>-78.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>07/09/24</td>\n",
       "      <td>50.24</td>\n",
       "      <td>50.24</td>\n",
       "      <td>49.96</td>\n",
       "      <td>50.09</td>\n",
       "      <td>11107321.0</td>\n",
       "      <td>50.11</td>\n",
       "      <td>30950</td>\n",
       "      <td>-0.003382</td>\n",
       "      <td>-60.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Open   High    Low  Close      Volume   VWAP Transactions  \\\n",
       "0    01/03/23  42.40  42.64  42.03  42.20  11990004.0  42.23        38830   \n",
       "1    01/04/23  42.85  42.95  42.52  42.83  13229545.0  42.81        42125   \n",
       "2    01/05/23  42.42  42.53  42.27  42.35   9324542.0  42.40        30234   \n",
       "3    01/06/23  42.66  43.55  42.45  43.52  10441417.0  43.26        29984   \n",
       "4    01/09/23  43.89  44.14  43.71  43.71  12426293.0  43.95        40155   \n",
       "..        ...    ...    ...    ...    ...         ...    ...          ...   \n",
       "375  07/02/24  49.33  49.60  49.26  49.57   9674116.0  49.43        29669   \n",
       "376  07/03/24  49.81  50.22  49.81  50.16   6989690.0  50.13        20772   \n",
       "377  07/05/24  50.62  50.62  50.18  50.48   7271284.0  50.40        27871   \n",
       "378  07/08/24  50.52  50.56  50.23  50.26   8229614.0  50.35        29295   \n",
       "379  07/09/24  50.24  50.24  49.96  50.09  11107321.0  50.11        30950   \n",
       "\n",
       "     Percent Change  Return  \n",
       "0          0.000000    0.00  \n",
       "1          0.014929  223.93  \n",
       "2         -0.011207 -170.62  \n",
       "3          0.027627  415.88  \n",
       "4          0.004366   67.54  \n",
       "..              ...     ...  \n",
       "375        0.002224   39.10  \n",
       "376        0.011902  209.71  \n",
       "377        0.006380  113.74  \n",
       "378       -0.004358  -78.20  \n",
       "379       -0.003382  -60.43  \n",
       "\n",
       "[380 rows x 10 columns]"
      ]
     },
     "execution_count": 1001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_etf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.00830701127374092)"
      ]
     },
     "execution_count": 1002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Return\n",
    "df_etf2['Return'].sum()\n",
    "\n",
    "# Monthly Returns\n",
    "df_returns.groupby([df_returns.Date.dt.year, df_returns.Date.dt.month]).agg({'Return ETF': 'sum'})\n",
    "\n",
    "# Volatility \n",
    "df_etf2['Percent Change'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7513488052596894)"
      ]
     },
     "execution_count": 1003,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sharpe Ratio\n",
    "\n",
    "hp= ((15000 + df_returns['Return ETF'].sum()) / 15000) - 1\n",
    "shp = hp * (252/365)\n",
    "\n",
    "# Calculate annualized volatility \n",
    "av = df_etf2['Percent Change'].std() * math.sqrt(252)\n",
    "\n",
    "sharpe_ratio = (shp-.03)/av\n",
    "sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving 10 day and 100 day average\n",
    "\n",
    "df_etf2['Percent Change'].rolling(window=10).mean()\n",
    "df_ndx['Percent Change'].rolling(window=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4832295901331863)"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beta calculations\n",
    "df_cov = pd.merge(df_ndx[['Date', 'Percent Change']], df_etf2[['Date', 'Percent Change']], on='Date')\n",
    "df_cov = df_cov.drop(columns={'Date'})\n",
    "\n",
    "variance = df_ndx['Percent Change'].var()\n",
    "covariance = df_cov.cov()['Percent Change_x']['Percent Change_y']\n",
    "\n",
    "covariance/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forex Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.004123491421716438)"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Return\n",
    "df_forex2['Return'].sum()\n",
    "\n",
    "# Cumulative Return\n",
    "df_returns.groupby([df_returns.Date.dt.year, df_returns.Date.dt.month]).agg({'Return Forex': 'sum'})\n",
    "\n",
    "# Daily Returns\n",
    "df_forex2[['Date', 'Return']]\n",
    "\n",
    "# Percent Change\n",
    "df_forex2[['Date', 'Close', 'Percent Change']]\n",
    "\n",
    "# Volatility\n",
    "df_forex2['Percent Change'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Trends API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = f'https://serpapi.com/search.json?engine=google_trends&q=nike&data_type=TIMESERIES&date=2023-01-01 2024-07-09&api_key={serpapi_key}'\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# Parses JSON string to Python Dictionary\n",
    "parsed = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = f'https://serpapi.com/search.json?engine=google_trends&q=hoka&data_type=TIMESERIES&date=2023-01-01 2024-07-09&api_key={serpapi_key}'\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# Parses JSON string to Python Dictionary\n",
    "parsed2= json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_metadata': {'id': '669065c5a3f4efb91d8602cd',\n",
       "  'status': 'Success',\n",
       "  'json_endpoint': 'https://serpapi.com/searches/93a8cb5ed7edb4e1/669065c5a3f4efb91d8602cd.json',\n",
       "  'created_at': '2024-07-11 23:07:49 UTC',\n",
       "  'processed_at': '2024-07-11 23:07:49 UTC',\n",
       "  'google_trends_url': 'https://trends.google.com/trends/api/explore?tz=420&req=%7B%22comparisonItem%22%3A%5B%7B%22keyword%22%3A%22nike%22%2C%22geo%22%3A%22%22%2C%22time%22%3A%222023-01-01+2024-07-09%22%7D%5D%2C%22category%22%3A0%2C%22property%22%3A%22%22%2C%22userConfig%22%3A%22%7BuserType%3A+%5C%22USER_TYPE_LEGIT_USER%5C%22%7D%22%7D',\n",
       "  'raw_html_file': 'https://serpapi.com/searches/93a8cb5ed7edb4e1/669065c5a3f4efb91d8602cd.html',\n",
       "  'prettify_html_file': 'https://serpapi.com/searches/93a8cb5ed7edb4e1/669065c5a3f4efb91d8602cd.prettify',\n",
       "  'total_time_taken': 4.98},\n",
       " 'search_parameters': {'engine': 'google_trends',\n",
       "  'q': 'nike',\n",
       "  'date': '2023-01-01 2024-07-09',\n",
       "  'tz': '420',\n",
       "  'data_type': 'TIMESERIES'},\n",
       " 'interest_over_time': {'timeline_data': [{'date': 'Jan 1\\u2009–\\u20097, 2023',\n",
       "    'timestamp': '1672531200',\n",
       "    'values': [{'query': 'nike', 'value': '78', 'extracted_value': 78}]},\n",
       "   {'date': 'Jan 8\\u2009–\\u200914, 2023',\n",
       "    'timestamp': '1673136000',\n",
       "    'values': [{'query': 'nike', 'value': '74', 'extracted_value': 74}]},\n",
       "   {'date': 'Jan 15\\u2009–\\u200921, 2023',\n",
       "    'timestamp': '1673740800',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Jan 22\\u2009–\\u200928, 2023',\n",
       "    'timestamp': '1674345600',\n",
       "    'values': [{'query': 'nike', 'value': '70', 'extracted_value': 70}]},\n",
       "   {'date': 'Jan 29\\u2009–\\u2009Feb 4, 2023',\n",
       "    'timestamp': '1674950400',\n",
       "    'values': [{'query': 'nike', 'value': '70', 'extracted_value': 70}]},\n",
       "   {'date': 'Feb 5\\u2009–\\u200911, 2023',\n",
       "    'timestamp': '1675555200',\n",
       "    'values': [{'query': 'nike', 'value': '70', 'extracted_value': 70}]},\n",
       "   {'date': 'Feb 12\\u2009–\\u200918, 2023',\n",
       "    'timestamp': '1676160000',\n",
       "    'values': [{'query': 'nike', 'value': '67', 'extracted_value': 67}]},\n",
       "   {'date': 'Feb 19\\u2009–\\u200925, 2023',\n",
       "    'timestamp': '1676764800',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Feb 26\\u2009–\\u2009Mar 4, 2023',\n",
       "    'timestamp': '1677369600',\n",
       "    'values': [{'query': 'nike', 'value': '72', 'extracted_value': 72}]},\n",
       "   {'date': 'Mar 5\\u2009–\\u200911, 2023',\n",
       "    'timestamp': '1677974400',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Mar 12\\u2009–\\u200918, 2023',\n",
       "    'timestamp': '1678579200',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Mar 19\\u2009–\\u200925, 2023',\n",
       "    'timestamp': '1679184000',\n",
       "    'values': [{'query': 'nike', 'value': '72', 'extracted_value': 72}]},\n",
       "   {'date': 'Mar 26\\u2009–\\u2009Apr 1, 2023',\n",
       "    'timestamp': '1679788800',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Apr 2\\u2009–\\u20098, 2023',\n",
       "    'timestamp': '1680393600',\n",
       "    'values': [{'query': 'nike', 'value': '75', 'extracted_value': 75}]},\n",
       "   {'date': 'Apr 9\\u2009–\\u200915, 2023',\n",
       "    'timestamp': '1680998400',\n",
       "    'values': [{'query': 'nike', 'value': '79', 'extracted_value': 79}]},\n",
       "   {'date': 'Apr 16\\u2009–\\u200922, 2023',\n",
       "    'timestamp': '1681603200',\n",
       "    'values': [{'query': 'nike', 'value': '80', 'extracted_value': 80}]},\n",
       "   {'date': 'Apr 23\\u2009–\\u200929, 2023',\n",
       "    'timestamp': '1682208000',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Apr 30\\u2009–\\u2009May 6, 2023',\n",
       "    'timestamp': '1682812800',\n",
       "    'values': [{'query': 'nike', 'value': '72', 'extracted_value': 72}]},\n",
       "   {'date': 'May 7\\u2009–\\u200913, 2023',\n",
       "    'timestamp': '1683417600',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'May 14\\u2009–\\u200920, 2023',\n",
       "    'timestamp': '1684022400',\n",
       "    'values': [{'query': 'nike', 'value': '74', 'extracted_value': 74}]},\n",
       "   {'date': 'May 21\\u2009–\\u200927, 2023',\n",
       "    'timestamp': '1684627200',\n",
       "    'values': [{'query': 'nike', 'value': '69', 'extracted_value': 69}]},\n",
       "   {'date': 'May 28\\u2009–\\u2009Jun 3, 2023',\n",
       "    'timestamp': '1685232000',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Jun 4\\u2009–\\u200910, 2023',\n",
       "    'timestamp': '1685836800',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Jun 11\\u2009–\\u200917, 2023',\n",
       "    'timestamp': '1686441600',\n",
       "    'values': [{'query': 'nike', 'value': '68', 'extracted_value': 68}]},\n",
       "   {'date': 'Jun 18\\u2009–\\u200924, 2023',\n",
       "    'timestamp': '1687046400',\n",
       "    'values': [{'query': 'nike', 'value': '65', 'extracted_value': 65}]},\n",
       "   {'date': 'Jun 25\\u2009–\\u2009Jul 1, 2023',\n",
       "    'timestamp': '1687651200',\n",
       "    'values': [{'query': 'nike', 'value': '64', 'extracted_value': 64}]},\n",
       "   {'date': 'Jul 2\\u2009–\\u20098, 2023',\n",
       "    'timestamp': '1688256000',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Jul 9\\u2009–\\u200915, 2023',\n",
       "    'timestamp': '1688860800',\n",
       "    'values': [{'query': 'nike', 'value': '68', 'extracted_value': 68}]},\n",
       "   {'date': 'Jul 16\\u2009–\\u200922, 2023',\n",
       "    'timestamp': '1689465600',\n",
       "    'values': [{'query': 'nike', 'value': '67', 'extracted_value': 67}]},\n",
       "   {'date': 'Jul 23\\u2009–\\u200929, 2023',\n",
       "    'timestamp': '1690070400',\n",
       "    'values': [{'query': 'nike', 'value': '72', 'extracted_value': 72}]},\n",
       "   {'date': 'Jul 30\\u2009–\\u2009Aug 5, 2023',\n",
       "    'timestamp': '1690675200',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Aug 6\\u2009–\\u200912, 2023',\n",
       "    'timestamp': '1691280000',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Aug 13\\u2009–\\u200919, 2023',\n",
       "    'timestamp': '1691884800',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Aug 20\\u2009–\\u200926, 2023',\n",
       "    'timestamp': '1692489600',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Aug 27\\u2009–\\u2009Sep 2, 2023',\n",
       "    'timestamp': '1693094400',\n",
       "    'values': [{'query': 'nike', 'value': '73', 'extracted_value': 73}]},\n",
       "   {'date': 'Sep 3\\u2009–\\u20099, 2023',\n",
       "    'timestamp': '1693699200',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Sep 10\\u2009–\\u200916, 2023',\n",
       "    'timestamp': '1694304000',\n",
       "    'values': [{'query': 'nike', 'value': '66', 'extracted_value': 66}]},\n",
       "   {'date': 'Sep 17\\u2009–\\u200923, 2023',\n",
       "    'timestamp': '1694908800',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Sep 24\\u2009–\\u200930, 2023',\n",
       "    'timestamp': '1695513600',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Oct 1\\u2009–\\u20097, 2023',\n",
       "    'timestamp': '1696118400',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Oct 8\\u2009–\\u200914, 2023',\n",
       "    'timestamp': '1696723200',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Oct 15\\u2009–\\u200921, 2023',\n",
       "    'timestamp': '1697328000',\n",
       "    'values': [{'query': 'nike', 'value': '69', 'extracted_value': 69}]},\n",
       "   {'date': 'Oct 22\\u2009–\\u200928, 2023',\n",
       "    'timestamp': '1697932800',\n",
       "    'values': [{'query': 'nike', 'value': '66', 'extracted_value': 66}]},\n",
       "   {'date': 'Oct 29\\u2009–\\u2009Nov 4, 2023',\n",
       "    'timestamp': '1698537600',\n",
       "    'values': [{'query': 'nike', 'value': '67', 'extracted_value': 67}]},\n",
       "   {'date': 'Nov 5\\u2009–\\u200911, 2023',\n",
       "    'timestamp': '1699142400',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Nov 12\\u2009–\\u200918, 2023',\n",
       "    'timestamp': '1699747200',\n",
       "    'values': [{'query': 'nike', 'value': '78', 'extracted_value': 78}]},\n",
       "   {'date': 'Nov 19\\u2009–\\u200925, 2023',\n",
       "    'timestamp': '1700352000',\n",
       "    'values': [{'query': 'nike', 'value': '100', 'extracted_value': 100}]},\n",
       "   {'date': 'Nov 26\\u2009–\\u2009Dec 2, 2023',\n",
       "    'timestamp': '1700956800',\n",
       "    'values': [{'query': 'nike', 'value': '85', 'extracted_value': 85}]},\n",
       "   {'date': 'Dec 3\\u2009–\\u20099, 2023',\n",
       "    'timestamp': '1701561600',\n",
       "    'values': [{'query': 'nike', 'value': '82', 'extracted_value': 82}]},\n",
       "   {'date': 'Dec 10\\u2009–\\u200916, 2023',\n",
       "    'timestamp': '1702166400',\n",
       "    'values': [{'query': 'nike', 'value': '85', 'extracted_value': 85}]},\n",
       "   {'date': 'Dec 17\\u2009–\\u200923, 2023',\n",
       "    'timestamp': '1702771200',\n",
       "    'values': [{'query': 'nike', 'value': '78', 'extracted_value': 78}]},\n",
       "   {'date': 'Dec 24\\u2009–\\u200930, 2023',\n",
       "    'timestamp': '1703376000',\n",
       "    'values': [{'query': 'nike', 'value': '79', 'extracted_value': 79}]},\n",
       "   {'date': 'Dec 31, 2023\\u2009–\\u2009Jan 6, 2024',\n",
       "    'timestamp': '1703980800',\n",
       "    'values': [{'query': 'nike', 'value': '69', 'extracted_value': 69}]},\n",
       "   {'date': 'Jan 7\\u2009–\\u200913, 2024',\n",
       "    'timestamp': '1704585600',\n",
       "    'values': [{'query': 'nike', 'value': '69', 'extracted_value': 69}]},\n",
       "   {'date': 'Jan 14\\u2009–\\u200920, 2024',\n",
       "    'timestamp': '1705190400',\n",
       "    'values': [{'query': 'nike', 'value': '63', 'extracted_value': 63}]},\n",
       "   {'date': 'Jan 21\\u2009–\\u200927, 2024',\n",
       "    'timestamp': '1705795200',\n",
       "    'values': [{'query': 'nike', 'value': '58', 'extracted_value': 58}]},\n",
       "   {'date': 'Jan 28\\u2009–\\u2009Feb 3, 2024',\n",
       "    'timestamp': '1706400000',\n",
       "    'values': [{'query': 'nike', 'value': '62', 'extracted_value': 62}]},\n",
       "   {'date': 'Feb 4\\u2009–\\u200910, 2024',\n",
       "    'timestamp': '1707004800',\n",
       "    'values': [{'query': 'nike', 'value': '63', 'extracted_value': 63}]},\n",
       "   {'date': 'Feb 11\\u2009–\\u200917, 2024',\n",
       "    'timestamp': '1707609600',\n",
       "    'values': [{'query': 'nike', 'value': '64', 'extracted_value': 64}]},\n",
       "   {'date': 'Feb 18\\u2009–\\u200924, 2024',\n",
       "    'timestamp': '1708214400',\n",
       "    'values': [{'query': 'nike', 'value': '66', 'extracted_value': 66}]},\n",
       "   {'date': 'Feb 25\\u2009–\\u2009Mar 2, 2024',\n",
       "    'timestamp': '1708819200',\n",
       "    'values': [{'query': 'nike', 'value': '62', 'extracted_value': 62}]},\n",
       "   {'date': 'Mar 3\\u2009–\\u20099, 2024',\n",
       "    'timestamp': '1709424000',\n",
       "    'values': [{'query': 'nike', 'value': '65', 'extracted_value': 65}]},\n",
       "   {'date': 'Mar 10\\u2009–\\u200916, 2024',\n",
       "    'timestamp': '1710028800',\n",
       "    'values': [{'query': 'nike', 'value': '64', 'extracted_value': 64}]},\n",
       "   {'date': 'Mar 17\\u2009–\\u200923, 2024',\n",
       "    'timestamp': '1710633600',\n",
       "    'values': [{'query': 'nike', 'value': '67', 'extracted_value': 67}]},\n",
       "   {'date': 'Mar 24\\u2009–\\u200930, 2024',\n",
       "    'timestamp': '1711238400',\n",
       "    'values': [{'query': 'nike', 'value': '68', 'extracted_value': 68}]},\n",
       "   {'date': 'Mar 31\\u2009–\\u2009Apr 6, 2024',\n",
       "    'timestamp': '1711843200',\n",
       "    'values': [{'query': 'nike', 'value': '68', 'extracted_value': 68}]},\n",
       "   {'date': 'Apr 7\\u2009–\\u200913, 2024',\n",
       "    'timestamp': '1712448000',\n",
       "    'values': [{'query': 'nike', 'value': '67', 'extracted_value': 67}]},\n",
       "   {'date': 'Apr 14\\u2009–\\u200920, 2024',\n",
       "    'timestamp': '1713052800',\n",
       "    'values': [{'query': 'nike', 'value': '71', 'extracted_value': 71}]},\n",
       "   {'date': 'Apr 21\\u2009–\\u200927, 2024',\n",
       "    'timestamp': '1713657600',\n",
       "    'values': [{'query': 'nike', 'value': '65', 'extracted_value': 65}]},\n",
       "   {'date': 'Apr 28\\u2009–\\u2009May 4, 2024',\n",
       "    'timestamp': '1714262400',\n",
       "    'values': [{'query': 'nike', 'value': '68', 'extracted_value': 68}]},\n",
       "   {'date': 'May 5\\u2009–\\u200911, 2024',\n",
       "    'timestamp': '1714867200',\n",
       "    'values': [{'query': 'nike', 'value': '63', 'extracted_value': 63}]},\n",
       "   {'date': 'May 12\\u2009–\\u200918, 2024',\n",
       "    'timestamp': '1715472000',\n",
       "    'values': [{'query': 'nike', 'value': '61', 'extracted_value': 61}]},\n",
       "   {'date': 'May 19\\u2009–\\u200925, 2024',\n",
       "    'timestamp': '1716076800',\n",
       "    'values': [{'query': 'nike', 'value': '63', 'extracted_value': 63}]},\n",
       "   {'date': 'May 26\\u2009–\\u2009Jun 1, 2024',\n",
       "    'timestamp': '1716681600',\n",
       "    'values': [{'query': 'nike', 'value': '63', 'extracted_value': 63}]},\n",
       "   {'date': 'Jun 2\\u2009–\\u20098, 2024',\n",
       "    'timestamp': '1717286400',\n",
       "    'values': [{'query': 'nike', 'value': '63', 'extracted_value': 63}]},\n",
       "   {'date': 'Jun 9\\u2009–\\u200915, 2024',\n",
       "    'timestamp': '1717891200',\n",
       "    'values': [{'query': 'nike', 'value': '63', 'extracted_value': 63}]},\n",
       "   {'date': 'Jun 16\\u2009–\\u200922, 2024',\n",
       "    'timestamp': '1718496000',\n",
       "    'values': [{'query': 'nike', 'value': '58', 'extracted_value': 58}]},\n",
       "   {'date': 'Jun 23\\u2009–\\u200929, 2024',\n",
       "    'timestamp': '1719100800',\n",
       "    'values': [{'query': 'nike', 'value': '61', 'extracted_value': 61}]},\n",
       "   {'date': 'Jun 30\\u2009–\\u2009Jul 6, 2024',\n",
       "    'timestamp': '1719705600',\n",
       "    'values': [{'query': 'nike', 'value': '61', 'extracted_value': 61}]},\n",
       "   {'date': 'Jul 7\\u2009–\\u200913, 2024',\n",
       "    'timestamp': '1720310400',\n",
       "    'values': [{'query': 'nike', 'value': '61', 'extracted_value': 61}]}]}}"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe to store their search interest\n",
    "df_google_trends = pd.DataFrame(columns=['Date', 'Nike', 'Hoka'])\n",
    "for nike, hoka in zip(parsed['interest_over_time']['timeline_data'], parsed2['interest_over_time']['timeline_data']):\n",
    "    values = nike['values']\n",
    "    values2 = hoka['values']\n",
    "    new_row = {'Date': nike['timestamp'], 'Nike': values[0]['value'],\n",
    "              'Hoka': values2[0]['value']}\n",
    "    df_google_trends = pd.concat([df_google_trends, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/62zqq87s4kgflwh21c1mktwh0000gn/T/ipykernel_65913/3905668183.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df_google_trends['Date'] = pd.to_datetime(df_google_trends['Date'], unit='s').dt.strftime('%m/%d/%y')\n"
     ]
    }
   ],
   "source": [
    "# Converts unix Msec timestamp to month/day/year format\n",
    "df_google_trends['Date'] = pd.to_datetime(df_google_trends['Date'], unit='s').dt.strftime('%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_trends.to_csv('GT_Table.csv', float_format='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['nike'] = df_nike2\n",
    "db['decker'] = df_decker2\n",
    "db['spotify'] = df_spotify2\n",
    "db['etf'] = df_etf2\n",
    "db['forex'] = df_forex2\n",
    "db['google_trends'] = df_google_trends\n",
    "db['ndx'] = df_ndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_var' (str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinwu/Documents/SkillStorm/.venv/lib/python3.12/site-packages/IPython/extensions/storemagic.py:229: UserWarning: This is now an optional IPython functionality, setting autorestore/test_var requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n"
     ]
    }
   ],
   "source": [
    "test_var = '123'\n",
    "%store test_var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
